Here's a step-by-step guide to install **k0s** on Ubuntu 20.04. **k0s** is a lightweight, zero-friction Kubernetes distribution by Mirantis, suitable for both development and production.
Link: https://docs.k0sproject.io/v1.21.0+k0s.0/k0s-multi-node/

### Step 1: Update the System

Start by updating your package lists and upgrading any packages:

```bash
sudo apt update && sudo apt upgrade -y
```

### Step 2: Download the k0s Binary

1. Download the latest k0s binary:

   ```bash
   curl -sSLf https://get.k0s.sh | sudo sh
   ```

2. **Verify Installation**:
   - Check that k0s has been installed correctly:

     ```bash
     k0s version
     ```

### Step 3: Install k0s as a Single-Node Cluster

To set up a single-node cluster, you’ll install k0s in controller mode with the `--single` flag.

1. **Install k0s** as a single-node controller:

   ```bash
   sudo k0s install controller --single
   ```

2. **Start k0s**:

   ```bash
   sudo k0s start
   ```

3. **Verify k0s Status**:

   ```bash
   sudo k0s status
   ```

### Step 4: Configure kubectl to Access the Cluster

k0s includes a configuration file for `kubectl` access. Set up `kubectl` by linking to the configuration file:

1. Create a directory for the Kubernetes config if it doesn’t exist:

   ```bash
   mkdir -p ~/.kube
   ```

2. Copy the k0s kubeconfig file:

   ```bash
   sudo cp /var/lib/k0s/pki/admin.conf ~/.kube/config
   sudo chown $(id -u):$(id -g) ~/.kube/config
   ```

3. **Verify Cluster Access**:

   Check the cluster status using `kubectl`:

   ```bash
   kubectl get nodes
   ```

   This should display your single-node cluster.

### Step 5: Enable k0s as a System Service (Optional)

To have k0s start automatically on system boot, use the following command:

```bash
sudo systemctl enable k0scontroller
```

### Step 6: Check Cluster Status
Install kubectl
```bash
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.28/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.28/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```
Install kubectl, kubeadmin and kublet:
```bash
sudo apt update
sudo apt install -y kubectl
```
Use the following command to verify that the k0s cluster is running and healthy:
```bash
kubectl get all --all-namespaces
```

### Optional: Adding Worker Nodes: You will not be able to add worker node to a single cluster

To add worker nodes to this controller, generate a token on the controller node:

```bash
sudo k0s token create --role=worker
```

Then, on each worker node, install k0s and use this token to join the worker to the cluster:

```bash
curl -sSLf https://get.k0s.sh | sudo sh
sudo k0s install worker --token <your-worker-token>
sudo k0s start
```

### Summary

- **Download** and install k0s.
- **Start k0s** as a single-node controller.
- **Set up kubectl** to interact with the cluster.
- Optionally, **enable k0s** as a system service and add worker nodes if needed.

Your single-node k0s Kubernetes cluster should now be up and running on Ubuntu 20.04.
=======================================================================================
## Script to install kubelet 
```bash
#!/bin/bash

# Update the package index and install dependencies
sudo apt-get update -y
sudo apt-get install -y apt-transport-https ca-certificates curl

# Add Kubernetes signing key
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

# Add the Kubernetes APT repository
cat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list
deb https://apt.kubernetes.io/ kubernetes-xenial main
EOF

# Update the package index with Kubernetes packages
sudo apt-get update -y

# Install kubelet, kubectl; you can add kubeadm if you choose
sudo apt-get install -y kubelet kubectl

# Hold the versions to prevent automatic upgrades
sudo apt-mark hold kubelet kubeadm kubectl

# Enable kubelet to start on boot
sudo systemctl enable kubelet
sudo systemctl start kubelet

echo "Kubelet, kubeadm, and kubectl have been installed successfully."
```

--------------------------------------------------------------

To add worker nodes, you need to set up `k0s` in **multi-node mode** by explicitly configuring it as a **controller** in a multi-node setup, rather than a single-node configuration.

---

### **Solution Steps**

1. **Reinitialize the Control Plane for a Multi-Node Setup**
   - If you initially set up `k0s` as a standalone node, you’ll need to reconfigure it as a multi-node controller.
   - **Stop and Remove the Current Single-Node Configuration**:
     ```bash
     sudo k0s stop
     sudo k0s reset
     ```
   - **Reinstall the Controller in Multi-Node Mode**:
     Reinstall `k0s` as a controller. The following command installs `k0s` in multi-node mode:
     ```bash
     sudo k0s install controller
     ```
   - **Start the Controller**:
     ```bash
     sudo k0s start
     ```

2. **Generate the Worker Join Token**
   - After reinitializing the control plane as a multi-node controller, try generating the worker join token again:
     ```bash
     sudo k0s token create --role=worker
     ```

3. **Use the Token to Join Worker Nodes**
   - Use the generated token on your worker nodes to join them to the cluster:
     ```bash
     sudo k0s install worker --token-file <token>
     sudo k0s start
     ```

---

### **Verification**

- After joining the worker nodes, you can check the cluster status on the control plane:
  ```bash
  sudo k0s kubectl get nodes
  ```
  This should display the controller and worker nodes in a “Ready” state.

---

These steps will configure your `k0s` setup to support a multi-node cluster, allowing worker nodes to join.



---
## Test Questions
1. How many pods exist on the system?
```
kubectl get pods
```
Count the number of pods (if any)

2. Create a new pod with the nginx image.
```
kubectl run nginx --image=nginx
```
3. How many pods are created now?
```
kubectl get pods
```
Count the number of pods (if any)

4. To get the system to tell you you can also do this
```
kubectl get pods --no-headers | wc -l
```
--no-headers should be obvious - output only the details.
wc is the word count program. -l tells it to count lines instead, and it will count the lines emitted by kubectl

5. What is the image used to create the new pods?
kubectl describe outputs lots of information. The following will describe all pods whose name starts with newpods, and then we filter with grep to get what we are looking for.
```
kubectl describe pod newpods | grep image
```
We see that all three are pulling the same image.

6 Which nodes are these pods placed on?
```
kubectl get pods -o wide
```
Note the node column for each of the 3 newpods pods

7. How many containers are part of the pod webapp?
```
kubectl describe pod webapp
```
Look under the Containers section. Note there is nginx and agentx

8. What images are used in the new webapp pod?
```
Examine the output from Q6. For each of the identified containers, look at Image:
```
9. What is the state of the container agentx in the pod webapp?
```
kubectl describe pod webapp
```
Examine the State: field for the agentx container.

10. Why do you think the container agentx in pod webapp is in error?
```
Examine the output from Q8 and look in the Events: section at the end. Look at the event that says failed to pull and unpack image ...
```
11. What does the READY column in the output of the kubectl get pods command indicate?
```
kubectl get pods
```
Look at the webapp pod which we know has two containers and one is in error. You can deduce which of the answers is correct from this.

12. Delete the webapp Pod.
```
kubectl delete pod webapp
```
To delete the pod without any delay and confirmation, we can add --force flag. Note that in a real production system, forcing is a last resort as it can leave behind containers that haven't been cleaned up properly. Some may have important cleanup jobs to run when they are requested to terminate, which wouldn't get run.

You can however use --force in the exam as it will gain you time. No exam pods rely on any of the above.

13. Create a new pod with the name redis and with the image redis123.
Use a pod-definition YAML file.
To create the pod definition YAML file:

--dry-run=client tells kubectl to test without actually doing anything. -o yaml says "Output what you would send to API server to the console", which we then redirect into the named file.
```
kubectl run redis --image=redis123 --dry-run=client -o yaml > redis.yaml
```
And now use the YAML you created to deploy the pod.
```
kubectl create -f redis.yaml
```
Now change the image on this pod to redis.
Once done, the pod should be in a running state.
There are three ways this can be done!

Method 1
Edit your manifest file created in last question
```
vi redis.yaml
```
Fix the image name in the redis.yaml to redis, save and exit.

Apply the edited yaml
```
kubectl apply -f redis.yaml
```
Method 2
Edit the running pod directly (note not all fields can be edited this way)
```
kubectl edit pod redis
```
This will bring the pod YAML up in vi. Edit it as per method 1. When you eixt vi the change will be immediately applied. If you make a mistake, you will be dropped back into vi

Method 3
Patch the image directly. For this you need to know the name of the container in the pod, as we assign the new image to that name, as in container_image_name=new_image
```
kubectl set image pod/redis redis=redis
```
Use a pod-definition YAML file.

Now change the image on this pod to redis.

Once done, the pod should be in a running state.
